<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Tensorflow基本概念汇总]]></title>
    <url>%2F2017%2F10%2F22%2Ftensorflow%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[基本概念——以下概念基本取自《TensorFlow实战Google深度学习框架》与各种无私博主 TensorFlow程序一般可以分为两个阶段， 构造部分，使用tensor表示数据，使用graph来表示计算任务 执行部分，在被称为Session的context里执行图中的计算（使用feed和fetch可以为任意的op赋值或从中获取数据） 计算图TensorFlow中每一个计算都是计算图上的一个节点。TensorFlow会自动生成一个默认的计算图，如果没有特殊指定，运算会自动加入这个计算图中。 张量在TensorFlow中，张量并没有真正保存数字，它只是对TensorFlow中运算结果的引用。一个张量中主要保存了三个属性：名字（name）、维度（shape）和类型（type）。张量的命名是通过“node:src_output”的形式来给出，node为节点的名称，src_ouput表示当前张量来自节点的第几个输出（编号从0开始）。当计算图构造完成后，可以通过会话（session）来得到张量的计算结果（或者借助eval()），如tf.Session().run(result)。 会话TensorFlow中的会话（session）用来执行定义好的运算。TensorFlow不会自动生成默认的会话，需要手动指定。 placeholder机制TensorFlow提供了placeholder机制用于提供输入数据：placeholder相当于定义了一个位置，这个位置的数据在程序运行时再使用feed_dict来指定。为什么会出现这个？若每轮迭代选取的数据都要通过常量来表示，计算图的节点就会非常多。（那为什么不适用variable呢？我现在还没想清楚） collection集合12tf.add_to_collection(&apos;losses&apos;, mse_loss)loss = tf.get_collection(&apos;losses&apos;)12 在一个计算图中，可以通过集合（collection）来管理不同类别的资源（可以是张量、变量或者运行Tensorflow程序所需要的队列资源等）。比如通过tf.add_to_collection函数可以将资源加入一个或多个集合中，然后通过tf.get_collection获取一个集合里面的所有资源。TensorFlow中也自动管理了一些最常用的集合，如：tf.GraphKeys.TRAINABLE_VARIABLES是可学习变量的集合，可通过tf.trainable_variables()获得。 见《TensorFlow实战Google深度学习框架》P42。 tf.constanta = tf.constant([1.0, 2.0], name=&#39;a&#39;)tf.constant是一个计算，这个计算的结果为一个张量，保存在变量a中。张量主要的三个属性：名字（name）、维度（shape）和类型（type）。见《TensorFlow实战Google深度学习框架》P43。 tf.matmula=tf.matmul(x, w1)tf.matmul实现了矩阵乘法的功能。 tf.Variable12weights = tf.Variable(tf.random_normal([2, 3], stddev=2))w2=tf.Variable(weights.initiallized_value())12 变量tf.Variable的作用就是保存和更新神经网络中的参数，也支持通过其他变量的初始值来初始化新的变量。tf.Variable是一个类。tensorflow随机数生成函数：tf.random_normal、tf.truncated_normal、tf.random_uniform、tf.random_gammatensorflow常数生成函数：tf.zeros、tf.ones、tf.fill、tf.constant见《TensorFlow实战Google深度学习框架》P43。 tf.get_variable() 和 tf.variable_scope()和tf.Variable不同的一点是，tf.get_variable 必须包含一个指定变量名称的参数，它会根据这个名字去创建或者获取变量。如果需要通过tf.get_variable获取一个已经创建的变量，需要通过tf.variable_scope函数生成一个上下文管理器。具体说来，当tf.variable_scope函数使用参数reuse=True生成上下文管理器时，这个上下文管理器内所有的tf.get_variable函数会直接获取已经创建的变量，如果变量不存在则tf.get_variable函数将报错；相反，如果tf.variable_scope函数使用参数reuse=False生成上下文管理器时，tf.get_variable操作将创建新的变量，如果同名的变量已经存在，则tf.get_variable函数将报错。说起来有点绕，可以看下书籍《TensorFlow实战Google深度学习框架》5.3节 变量管理。想一下，为什么这么做呢？答：这种变量管理的方式就不需要将所有变量都作为参数传递到不同的函数中了，并且大大提高程序的可读性。除此之外，tf.variable_scope函数也会创建一个命名空间，在命名空间内创建的变量名称都会带上这个命名空间名作为前缀。tf.name_scope也会创建一个命名空间，但和前者的区别在于此函数将会自动忽略tf.get_variable 创建的变量。参见：《TensorFlow实战Google深度学习框架》5.3节 变量管理 ；What’s the difference of name scope and a variable scope in tensorflow? tf.assignassign(ref, value, validate_shape=None, use_locking=None, name=None)tf.assign用来更新模型中变量的值，效果等同于 ref = value。 tf.train.ExponentialMovingAverage滑动平均1234ema = tf.train.ExponentialMovingAverage(decay, num_updates) # 定义了一个滑动平均类的对象，初始化时给定了衰减率deay和控制衰减率的变量num_updatesmaintain_averages_op = ema.apply([v1]) #定义一个更新变量滑动平均的操作，每次执行这个操作时参数中的变量都会被更新sess.run(maintain_averages_op) # 更新v1的滑动平均值v1_shadow = ema.average(v1) # 获取滑动平均之后变量的取值 1234 滑动平均不会改变变量本身的取值，而是会维护一个影子变量来记录其滑动平均值。所以当需要使用这个滑动平均值时，需要明确调用average函数。参见《TensorFlow实战Google深度学习框架》P90 tf.contrib.layer.l2_regularizer 正则化TensorFlow提供了tf.contrib.layer.l2_regularizer 函数，它可以返回一个函数，这个函数可以计算一个给定参数的L2正则化的值。 1loss_regular = tf.contrib.layer.l2_regularizer(lambda)(weights) # lambda为正则化项的权重，weights为需要正则化的值1 参见《TensorFlow实战Google深度学习框架》P88 tf.train.exponential_decay 指数衰减学习率12learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, mnist.train.num_examples/BATCH_SIZE, LEARNING_RATE_DECAY) # (基础学习率，当前迭代轮数，衰减速度（通常为过完一次所有训练数据需要的迭代次数），学习率衰减系数)train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(my_loss, global_step=global_step) # 使用指数衰减的学习率。在minimize函数中传入global_step将自动更新global_step参数，从而使得学习率也得到相应更新。12 衰减公式：learning_rate=learning_rate_base∗decay_rate(global_step/decay_steps) 参见《TensorFlow实战Google深度学习框架》P85 placeholder机制为什么会出现这个？若每轮迭代选取的数据都要通过常量来表示，计算图的节点就会非常多。为了避免这个问题，TensorFlow提供了placeholder机制用于提供输入数据：placeholder相当于定义了一个位置，这个位置的数据在程序运行时再使用feed_dict来指定。 12x = tf.placeholder(tf.float32, shape=(3, 2), name=&quot;input&quot;)sess.run(y, feed_dict=&#123;x:[[0.7, 0.9], [0.1, 0.4], [0.5, 0.8]]&#125;)12 tf.control_dependencies控制依赖1234with g.control_dependencies([a, b, c]): # `d` 和 `e` 将在 `a`, `b`, 和`c`执行完之后运行 d = … e = … 1234 在训练神经网络模型时，每过一遍数据既要通过反向传播来更新神经网络中的参数，又要更新每一个参数的滑动平均值。为了一次完成多个操作，TensorFlow提供了tf.control_dependencies和tf.group两种机制。下面两行程序和train_op = tf.group(train_step, variables_averages_op)是等价的。 12with tf.control_dependencies([train_step, variables_averages_op]): train_op = tf.no_op(name=&apos;train&apos;)12 tf.nn.sparse_softmax_cross_entropy_with_logits交叉熵函数1cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(y, tf.argmax(y_, 1))1 当分类问题只有一个正确答案时，可以使用这个函数来加速交叉熵的计算。这个函数的第一个参数时神经网络不包括softmax层的前向传播结果，第二个是训练数据的正确答案。因为标准答案是一个长度为10的一维数组，而该函数需要提供的是一个正确答案的数字，所以需要使用tf.argmax函数来得到正确答案对应的类别编号。 tf.train.GradientDescentOptimizer优化算法1train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)1 使用其来优化损失函数，注意在这个函数中会把global_step的值加1。 tf.train.Saver()保存加载模型保存模型： 12saver = tf.train.Saver()saver.save(sess, path_to_save, global_step=global_step)12 通过变量重命名的方式加载部分模型： 1234variable_averages = tf.train.ExponentialMovingAverage(mnist_train.MOVING_AVERAGE_DECAY) variables_to_restore = variable_averages.variables_to_restore() # Returns a map of names to Variables to restoresaver = tf.train.Saver(variables_to_restore) #参数可以是一个dict(即这里的用法)也可以是一个listsaver.restore(sess, path_saved)1234 参考：变量:创建、初始化、保存和加载；variables_to_restore(moving_avg_variables=None)]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[完成Hexo+Next的博客搭建]]></title>
    <url>%2F2017%2F10%2F05%2F%E8%AE%B0%E5%BD%95_%E5%AE%8C%E6%88%90Hexo%2BNext%E7%9A%84%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[耗时：一天成果：完成独立博客搭建（依附于GitHub）过程：网上搜集资料——&gt;配置GitHub——&gt;买域名，绑定———&gt;部署HEXO——&gt;优化Next主题中间经历的最崩溃的就是失误将错误的配置文件部署到GitHub上，导致找不出404原因，导致重新做了一边，太粗心哎我好蠢，一定要在本地试运行才可以！不过看着成功的运行起来，还是蛮有成就感哒。 安装和配置Hexo及Github安装Hexo安装Hexo前，需要安装以下： Node.js 下载地址 Git 下载地址 这里用迅雷下速度比较快，给的链接是64位的 如果要32位的自行百度这里提供个githup for win的图形化客户端，百度云下载：Git for win 但是并不建议使用。。玩git 还是用git bash 方式比较高大上~~ 如果已经安装完成以上程序，打开Git-bash，输入 1npm install -g hexo-cli 即可完成Hexo的安装。 使用Hexo进行本地建站选择一个本地的文件夹，如C:\hexo 输入 123hexo init C:/hexocd C:/hexonpm install 如果hexo安装成功，则在C:\hexo文件夹下的文件目录为 12345678.├── _config.yml // 网站的配置信息，你可以在此配置大部分的参数。├── package.json ├── scaffolds // 模板文件夹。当你新建文章时，Hexo会根据scaffold来建立文件。├── source // 存放用户资源的地方| ├── _drafts| └── _posts└── themes // 存放网站的主题。Hexo会根据主题来生成静态页面。 详细文件或文件夹的具体含义见 Hexo官方文档 之后输入 1hexo server 此时会启动本地部署好的默认的博客网站 地址是：http://localhost:4000/不出意外 这里应该是没啥问题的。。 创建Github账号访问Github官网进行注册 ，这里没啥好说的。 创建与账号同名的Repository一定要同名的Repository，比如帐号是myid,那新建的Repository名称应该是myid.github.io 配置SSH(1) 生成SSH检查是否已经有SSH Key，打开Git Bash，输入 cd ~/.ssh如果没有这个目录，则生成一个新的SSH，输入 1ssh-keygen -t rsa -C &quot;your e-mail&quot; 其中，your e-mail是你注册Github时用到的邮箱。 然后接下来几步都直接按回车键，最后生成如下 (2) 复制公钥内容到Github账户信息中打开~/.ssh/id_rsa.pub文件，复制里面的内容； 打开Github官网，登陆后进入到个人设置(点击头像-&gt;setting)，点击右侧的SSH Keys，点击Add SSH key；填写title之后，将之前复制的内容粘贴到Key框中，最后点击Add key即可。 (3) 测试SSH是否配置成功输入 1ssh -T git@github.com 如果显示以下，则说明ssh配置成功。 1Hi username! You&apos;ve successfully authenticated, but GitHub does not provide shell access. (4) 配置github 账户12git config --global user.name &quot;username&quot;git config --global user.email &quot;email&quot; 上面都是自己的用户名与邮箱 配置完之后输入： 1git config --list查看已设配置 将网站发布到Github的同名repository中 打开C:\Hexo文件夹中的_config.yml文件，找到如下位置，填写 12345# Deployment## Docs: http://hexo.io/docs/deployment.htmldeploy: type: git repo: git@github.com:MyGithub/MyGithub.github.io 到这基本上开始就没问题了，访问 MyGitHub.github.io 就是Hexo 首页了，下面主题就根据个人喜欢吧，我用的是Next，你也可以搜集一些其他出色的主题进行部署。可以参考 https://hexo.io/themes/ ====================================================================2017.10.06 今天顺手查了下主题优化，完成以下项目： 1234文章评论区：由于网易云跟帖服务已经挂了（- -！），现在使用 来必力做第三方，注册下，在配置文件中加上你的 U_ID 就好了。文章底层加阴影。添加访问人数与访问次数。添加菜单：分类、搜索、标签。 目前大概就这样吧，之后有空再找找其他优化。]]></content>
      <categories>
        <category>GitHub相关</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
</search>
